{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "\n",
    "import torch\n",
    "\n",
    "from indxr import Indxr\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        main: Indxr = None,\n",
    "        others: List[Indxr] = None,\n",
    "        callback: Callable = None,\n",
    "    ):\n",
    "        self.main = main\n",
    "        self.others = others\n",
    "        self.callback = callback\n",
    "\n",
    "    # Support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index: int) -> str:\n",
    "        if self.callback:\n",
    "            if self.others:\n",
    "                return self.callback(self.main[index], self.others)\n",
    "                \n",
    "            return self.callback(self.main[index])\n",
    "            \n",
    "        return self.main[index]\n",
    "\n",
    "    # This allows to call len(dataset) to get the dataset size\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Indxr(kind=\"jsonl\", path=\"data/queries.jsonl\", key_id=\"q_id\")\n",
    "b = Indxr(kind=\"jsonl\", path=\"data/users.jsonl\", key_id=\"u_id\")\n",
    "c = Indxr(kind=\"jsonl\", path=\"data/docs.jsonl\", key_id=\"d_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_something(query, others):\n",
    "    users, docs = others\n",
    "\n",
    "    pos_docs = docs.mget(query[\"pos_doc_ids\"])\n",
    "    pos_docs = [doc[\"text\"] for doc in pos_docs]\n",
    "\n",
    "    neg_docs = docs.mget(query[\"neg_doc_ids\"])\n",
    "    neg_docs = [doc[\"text\"] for doc in neg_docs]\n",
    "\n",
    "    user = users.get(query[\"user_id\"])\n",
    "    user_docs = docs.mget(user[\"doc_ids\"])\n",
    "    user_docs = [doc[\"text\"] for doc in user_docs]\n",
    "\n",
    "    return query[\"text\"], pos_docs, neg_docs, user_docs\n",
    "\n",
    "\n",
    "dataset = Dataset(\n",
    "    main=a,\n",
    "    others=[\n",
    "        b,\n",
    "        c,\n",
    "    ],\n",
    "    callback=do_something,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    # prefetch_factor=2,\n",
    ")\n",
    "\n",
    "for _ in tqdm(train_dataloader):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('indxr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8cd45eefef40dc68b1d8412f39a79cc1e728025c105896007f1114be7226c9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
